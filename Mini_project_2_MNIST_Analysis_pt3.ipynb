{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project #2: MNIST Analysis\n",
    "\n",
    "An easy-to-follow scikit-learn tutorial that will help you to get started with the Python machine learning.\n",
    "\n",
    "## Machine Learning with Python\n",
    "\n",
    "Machine learning is a branch in computer science that studies the design of algorithms that can learn.\n",
    "\n",
    "Typical tasks are concept learning, function learning or “predictive modeling”, clustering and finding predictive patterns. These tasks are learned through available data that were observed through experiences or instructions, for example.\n",
    "\n",
    "The hope that comes with this discipline is that including the experience into its tasks will eventually improve the learning. But this improvement needs to happen in such a way that the learning itself becomes automatic so that humans like ourselves don’t need to interfere anymore is the ultimate goal.\n",
    "\n",
    "Today’s scikit-learn tutorial will introduce you to the basics of Python machine learning:\n",
    "\n",
    "- Part 1: You'll learn how to use Python and its libraries to explore your data with the help of matplotlib and Principal Component Analysis (PCA),\n",
    "- Part 2a: And you'll preprocess your data with normalization and you'll split your data into training and test sets.\n",
    "- Part 2b: Next, you'll work with the well-known KMeans algorithm to construct an unsupervised model, fit this model to your data, predict values, and validate the model that you have built.\n",
    "- Part 3: As an extra, you'll also see how you can also use Support Vector Machines (SVM) to construct another model to classify your data.\n",
    "\n",
    "Let's move to part 3 now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classification\n",
    "\n",
    "When you recapped all of the information that you gathered out of the data exploration, you saw that you could build a model to predict which group a digit belongs to without you knowing the labels. And indeed, you just used the training data and not the target values to build your KMeans model.\n",
    "\n",
    "### Read-in Data\n",
    "\n",
    "For starters, let's import the `digits` dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import `datasets` from `sklearn`.\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# read `digits` using `load_digits()` into a variable called `digits`\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data and Specify the Model\n",
    "\n",
    "Then let's pick it up from the case where you use both the `digits` training data and the corresponding `target` values to build your model.\n",
    "\n",
    "If you follow the algorithm map, you’ll see that the first model that you meet is the linear SVC - SVC stands for Support Vector Classifier. Let’s apply this now to the `digits` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets \n",
    "# Split `data` into `X_train` and `X_test`\n",
    "# Split `target` into `y_train` and `y_test`\n",
    "# Split `images` into `images_train` and `images_test`\n",
    "# use `test_size` of 0.25 and `random_state` as 2019\n",
    "# if you have any doubts about this, please refer to part 2 of this project\n",
    "\n",
    "X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=2019)\n",
    "\n",
    "# Import the `svm` model from `sklearn`\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "# Create the SVC model by calling `svm.SVC()`\n",
    "# the SVC model has a series of hyper-parameters\n",
    "# for now, we are going to use the basics of the hyperparameters as follows\n",
    "# set `gamma` to 0.001 - `gamma` typically ranges from [0.001, 0.01]\n",
    "# set `C` to 100. , and then set `kernel` to 'linear'\n",
    "# name the model as `svc_model`\n",
    "svc_model = svm.SVC(gamma = 0.001, C=100, kernel='linear')\n",
    "\n",
    "# Fit the data to the SVC model\n",
    "# since this is supervised learning, you need to `fit` both `X_train` and `y_train` to it\n",
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Optimization\n",
    "\n",
    "You see here that you make use of `X_train` and `y_train` to fit the data to the `svc_model`. This is clearly different from clustering we did in part 2. Note also that in this example, you set the value of `gamma` manually. It is possible to automatically find good values for the parameters by using tools such as __grid search__ in combination with __cross validation__.\n",
    "\n",
    "Even though this is not the focus of this tutorial, you will see how you could have gone about this if you would have made use of grid search to adjust your parameters. You would have done something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for training data: 0.9844097995545658\n",
      "Best `C`: 10\n",
      "Best kernel: rbf\n",
      "Best `gamma`: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Split the `digits` data into two equal sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(digits.data, digits.target,test_size=0.5, random_state=2019)\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set the parameter candidates\n",
    "parameter_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf', 'sigmoid', 'poly']},\n",
    "]\n",
    "\n",
    "# Create a classifier with the parameter candidates\n",
    "\n",
    "\n",
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)\n",
    "\n",
    "# Train the classifier on training data\n",
    "\n",
    "\n",
    "clf.fit(X_train1, y_train1)\n",
    "\n",
    "# Print out the results \n",
    "print('Best score for training data:', clf.best_score_)\n",
    "print('Best `C`:',clf.best_estimator_.C)\n",
    "print('Best kernel:',clf.best_estimator_.kernel)\n",
    "print('Best `gamma`:',clf.best_estimator_.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a separate lecture on __hyperparameter optimization__ later in this class.\n",
    "\n",
    "Next, you use the classifier with the classifier and parameter candidates that you have just created to apply it to the __test part__ of your data set. You will also train a new classifier using the best parameters found by the grid search. You record the result to see if the best parameters that were found in the grid search are actually working by outperforming the initial model we created (`svc_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9822222222222222\n",
      "0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "# Apply the classifier to the test data, and view the accuracy score\n",
    "print(svc_model.score(X_test, y_test))\n",
    "\n",
    "# Train and score a new classifier with the grid search parameters\n",
    "print(svm.SVC(C=10, kernel='rbf', gamma=0.001).fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters indeed work well!\n",
    "\n",
    "Now what does this new knowledge tell you about the SVC classifier that you had modeled before you had done the grid search?\n",
    "\n",
    "Let’s back up to the model that you had made before.\n",
    "\n",
    "You see that in the SVM classifier, the penalty parameter `C` of the error term is specified at `100.`. Lastly, you see that the `kernel` has been explicitly specified as a `linear` one. The `kernel=` argument specifies the kernel type that you’re going to use in the algorithm and by default, this is `rbf`. In other cases, you can specify others such as `linear`, `poly`, …\n",
    "\n",
    "But what is a kernel exactly?\n",
    "\n",
    "A kernel is a similarity function, which is used to compute __similarity__ between the  __data points in the training set__. When you provide a kernel to an algorithm, together with the training data and the labels, you will get a classifier, as is the case here. You apply a trained model by assigning new unseen objects to it and let it classify them into predefined classes. For the SVM, you will typicall try to linearly divide your data points.\n",
    "\n",
    "However, the grid search tells you that an `rbf` kernel would’ve worked better. The penalty parameter and the gamma were also specified correctly.\n",
    "\n",
    "For now, let’s just say you just continue with a `linear` kernel and predict the values for the test set (`X_test`) and then comparing the predictive results with the original labels (`y_test`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 9 2 9 6 6 5 2 4 1 9 3 1 5 1 6 9 7 5 7 3 6 7 0 5 8 1 5 6 4 7 6 7 6 4 9 8\n",
      " 1 5 8 9 1 7 0 4 9 0 3 5 9 3 7 4 0 8 8 3 6 7 5 5 0 3 2 4 3 2 6 8 3 3 8 6 6\n",
      " 3 0 5 2 5 0 7 5 0 1 3 5 6 3 3 7 0 3 6 8 5 8 3 9 6 7 7 4 7 9 1 9 6 4 6 8 8\n",
      " 1 2 4 0 0 1 7 0 3 0 6 3 7 2 4 5 5 2 0 0 1 0 4 3 8 8 3 2 8 5 2 8 4 7 4 8 7\n",
      " 3 2 9 6 1 3 6 2 6 7 8 5 3 8 2 1 3 0 1 1 2 4 6 9 3 6 1 3 6 9 6 4 8 2 9 9 5\n",
      " 9 2 7 6 7 2 7 4 5 1 5 6 8 3 3 9 7 6 6 5 0 9 5 3 4 9 8 5 3 8 3 6 2 5 6 4 4\n",
      " 8 2 3 3 8 2 9 8 1 4 3 8 3 0 0 0 7 1 5 5 9 7 0 2 6 1 8 7 0 4 6 1 0 5 4 8 0\n",
      " 6 4 3 8 1 4 8 9 6 1 0 7 7 6 1 7 7 2 8 7 9 2 1 7 6 4 5 0 0 9 3 5 5 2 8 0 9\n",
      " 7 3 1 3 9 6 6 1 5 0 1 3 1 0 2 8 0 1 7 4 9 1 1 1 7 3 4 2 5 6 3 4 5 5 5 5 2\n",
      " 2 8 9 2 2 1 9 6 2 8 0 1 1 4 4 2 1 4 7 5 7 9 4 2 6 0 9 4 8 9 8 0 8 1 2 9 2\n",
      " 1 4 8 2 1 6 0 7 3 2 3 3 8 2 8 5 9 7 8 9 3 0 7 6 4 5 5 6 9 6 1 2 3 4 6 9 0\n",
      " 2 3 9 7 0 8 5 3 0 7 0 0 7 9 4 0 7 8 3 9 0 4 0 5 7 3 5 1 5 3 7 9 9 4 0 9 2\n",
      " 5 4 0 2 5 6]\n",
      "[5 9 2 9 6 6 5 2 4 1 5 3 1 5 1 6 9 7 5 7 3 6 7 0 5 8 1 5 6 4 7 6 3 6 4 9 8\n",
      " 1 5 8 9 1 7 0 4 9 0 3 5 9 3 7 4 0 9 8 3 6 7 5 5 0 3 2 4 3 2 6 8 3 3 8 6 6\n",
      " 3 0 5 2 5 0 7 5 0 1 3 5 6 3 3 7 0 3 6 8 5 8 3 9 6 9 8 4 7 9 1 9 6 4 6 8 8\n",
      " 1 2 4 0 0 1 7 0 3 0 6 3 7 2 4 5 5 2 0 0 1 0 4 3 8 8 3 2 8 5 2 8 4 7 4 8 7\n",
      " 3 2 9 6 1 3 6 2 6 7 8 5 3 8 2 1 3 0 1 1 2 4 6 9 3 6 1 3 6 9 6 4 8 2 9 9 5\n",
      " 9 2 7 6 7 2 7 4 5 1 5 6 8 3 3 9 7 6 6 5 0 9 5 3 4 9 8 5 3 8 3 6 2 8 6 4 4\n",
      " 8 2 3 3 8 2 9 8 1 4 3 8 3 0 0 0 7 1 5 5 9 7 0 2 6 1 8 7 0 4 6 1 0 5 4 8 0\n",
      " 6 4 3 8 1 4 8 9 6 1 0 7 7 6 1 7 7 2 8 7 9 2 1 7 6 4 5 0 0 9 3 5 5 2 8 0 9\n",
      " 7 3 1 3 9 6 6 1 5 0 1 3 1 0 2 8 0 1 7 4 9 1 1 1 7 3 4 2 5 6 3 4 5 5 5 5 2\n",
      " 2 8 9 2 2 1 9 6 2 8 0 1 1 4 4 2 1 4 7 5 7 9 4 2 6 0 9 4 8 9 8 0 8 1 2 9 2\n",
      " 1 4 8 2 1 6 0 7 3 2 3 3 8 2 8 5 9 7 8 9 3 0 7 6 4 5 5 6 9 6 9 2 3 4 6 9 0\n",
      " 2 3 9 7 0 8 5 3 0 7 0 0 7 9 4 0 7 8 3 9 0 4 0 5 7 3 5 1 5 3 7 9 9 4 0 9 2\n",
      " 5 4 0 2 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Predict the label of `X_test` using `.predict()`\n",
    "\n",
    "print(svc_model.predict(X_test))\n",
    "\n",
    "# Print `y_test` to check the results\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability of the result comparison is not very high, correct? Maybe visualizations, as they _always help improving readability_ of your data, would help?\n",
    "\n",
    "Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Assign the predicted values of `X_test` to `y_pred` using `.predict()`\n",
    "\n",
    "y_pred = svc_model.predict(X_test)\n",
    "\n",
    "# Zip together the `images_test` and `y_pred` values in `images_and_predictions`\n",
    "# Remember ot make the zipped result a list using list()\n",
    "\n",
    "images_and_predictions = list(zip(images_test, y_pred))\n",
    "\n",
    "# For the first 4 elements in `images_and_predictions`\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    # Initialize subplots in a grid of 1 by 4 at positions i+1\n",
    "    plt.subplot(1, 4, index + 1)\n",
    "    # Don't show axes\n",
    "    plt.axis('off')\n",
    "    # Display images in all subplots in the grid\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    # Add a title to the plot\n",
    "    plt.title('Predicted: ' + str(prediction))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plot is very similar to the plot that you made when we were doing the EDA in part 1 of this project. Only this time, you zip together the images and the __predicted values__ (instead of original labels) and you only take the first `4` elements of `images_and_predictions`.\n",
    "\n",
    "And as humans, you have to admit that these images are not that easy to classify yourselves, agreed?\n",
    "\n",
    "### More on Model Evaluation\n",
    "\n",
    "But now the biggest question: how does this model perform? Even though you see the accuracy at `98%` a few code blocks above, remember that is for your __training data__ (we call them __training accuracies__). The training accuracy of your model can easily go up to `100%` since the training process is to improve the __training accuracy__ to the extent possible. But we do not evaluate the models on the __training accuracy__, instead we will use the __test accuracy__ when you apply your trained model on your __test data__.\n",
    "\n",
    "__NOTE__: as a matter of fact, if your __training accuracy__ is too high, and your __test accuracy__ is much lower, you should check your model for __overfitting__. You do __NOT__ want an overfitted model! That's why sometimes we split the data into _training, testing, and validation_ sets.\n",
    "\n",
    "Let's now check how our model actually performed on the __test set__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        47\n",
      "          1       0.98      1.00      0.99        41\n",
      "          2       1.00      1.00      1.00        42\n",
      "          3       1.00      0.98      0.99        52\n",
      "          4       1.00      1.00      1.00        40\n",
      "          5       0.98      0.96      0.97        48\n",
      "          6       1.00      1.00      1.00        47\n",
      "          7       0.93      1.00      0.97        42\n",
      "          8       0.98      0.96      0.97        45\n",
      "          9       0.96      0.93      0.95        46\n",
      "\n",
      "avg / total       0.98      0.98      0.98       450\n",
      "\n",
      "[[47  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 41  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 42  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 51  0  0  0  1  0  0]\n",
      " [ 0  0  0  0 40  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 46  0  0  0  2]\n",
      " [ 0  0  0  0  0  0 47  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 42  0  0]\n",
      " [ 0  0  0  0  0  1  0  1 43  0]\n",
      " [ 0  1  0  0  0  0  0  1  1 43]]\n"
     ]
    }
   ],
   "source": [
    "# from `metrics` import `classification_report` and `confusion_matrix`\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Print the classification report using comparison of `y_test` and `y_pred`\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix using comparison of `y_test` and `y_pred`\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we used the confusure matrix in part 2 when we evaluate the clustering results, right? We say that we want the higher values to be on the top-left to bottom-right diagonal line. \n",
    "\n",
    "You clearly see that this model performs a whole lot better than the clustering model that you used earlier.\n",
    "\n",
    "You can also see it when you visualize the predicted and the actual labels with the help of `Isomap()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `Isomap()`\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# Create an isomap and fit the `digits` data to it\n",
    "X_iso = Isomap(n_neighbors=10).fit_transform(X_train)\n",
    "\n",
    "# Compute cluster centers and predict cluster index for each sample\n",
    "predicted = svc_model.predict(X_train)\n",
    "\n",
    "# Create a plot with subplots in a grid of 1X2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Adjust the layout\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "# Add scatterplots to the subplots \n",
    "ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=predicted)\n",
    "ax[0].set_title('Predicted labels')\n",
    "ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train)\n",
    "ax[1].set_title('Actual Labels')\n",
    "\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Predicted versus actual labels', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll see that this visualization confirms your classification report, which is very good news. :)\n",
    "\n",
    "### Do It Yourself\n",
    "\n",
    "Remember we used __grid search__ to get a set of 'best' hyperparameters of the model? Use these parameters to create another SVC model, and then train it with `X_train` and `y_train`, and apply it to `X_test` to evaluate its performance. \n",
    "\n",
    "In order to do that, you will nned to:\n",
    "- specify a SVC model using `C=10, kernel='rbf', gamma=0.001` as hyperparameters, then `fit` the model to `X_train` and `y_train`;\n",
    "- predict the labels by calling `.predict()` on `X_test`;\n",
    "- visualize the original image and predicted labels;\n",
    "- print out `confusion_matrix` and `classification_report`;\n",
    "- use `Isomap()` to visualize the classification results.\n",
    "\n",
    "Please write your code in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and score a new classifier with the grid search parameters\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2, images_train2, images_test2 = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=2019)\n",
    "print(svm.SVC(C=10, kernel='rbf', gamma=0.001).fit(X_train2, y_train2).score(X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C = 10, kernel = 'rbf', gamma = .001)\n",
    "\n",
    "clf.fit(X_train2, y_train2)\n",
    "\n",
    "y_pred2 = clf.predict(X_test2)\n",
    "\n",
    "#show the predictions for the last 4 images\n",
    "image_with_prediction = list(zip(images_test2, y_pred2))\n",
    "\n",
    "for idx, (image, pred) in enumerate(image_with_prediction[-4:]):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Prediction: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from `metrics` import `classification_report` and `confusion_matrix`\n",
    "from sklearn import metrics\n",
    "\n",
    "# Print the classification report using comparison of `y_test` and `y_pred`\n",
    "print(metrics.classification_report(y_test2, y_pred2))\n",
    "\n",
    "# Print the confusion matrix using comparison of `y_test` and `y_pred`\n",
    "print(metrics.confusion_matrix(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `Isomap()`\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# Create an isomap and fit the `digits` data to it\n",
    "X_iso = Isomap(n_neighbors=10).fit_transform(X_train2)\n",
    "\n",
    "# Compute cluster centers and predict cluster index for each sample\n",
    "predicted = svc_model.predict(X_train2)\n",
    "\n",
    "# Create a plot with subplots in a grid of 1X2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Adjust the layout\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "# Add scatterplots to the subplots \n",
    "ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=predicted)\n",
    "ax[0].set_title('Predicted labels')\n",
    "ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train2)\n",
    "ax[1].set_title('Actual Labels')\n",
    "\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Predicted versus actual labels', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer this question:\n",
    "\n",
    "Observe the results, and compare the results to the ones we obtained using `svc_model`. Is there any difference? Provide your answer in the block below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs appear to be flipped from left to right in the second results.  The precision, recall and f-1 score all improved by .01%, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for mini project 2. Please make sure your sync the complete notebook to your github repo for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
